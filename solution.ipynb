{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6efde7",
   "metadata": {},
   "source": [
    "# –≠—Ç–∞–ø 1. –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (EDA)\n",
    "\n",
    "üìä –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: 3262\n",
    "- –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å: 0.00\n",
    "- –°—Ä–µ–¥–Ω—è—è –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å: 255.01\n",
    "- –ú–µ–¥–∏–∞–Ω–Ω–∞—è –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å: 209.11\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å: 3943.33\n",
    "- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: 219.64\n",
    "- –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏: 86.13%\n",
    "\n",
    "<img src=\"static/–ö–∞–ª–æ—Ä–∏–∏.jpg\" width=\"1200\" height=\"400\">\n",
    "\n",
    "<img src=\"static/–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è.jpg\" width=\"1800\" height=\"300\">\n",
    "\n",
    "–í—ã–≤–æ–¥—ã:\n",
    "- –ï—Å—Ç—å —Å–∏–ª—å–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø–æ –∫–∞–ª–æ—Ä–∏—è–º–∏, –ø–æ—ç—Ç–æ–º—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç—Å–µ—á—å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å. –û–Ω–∏ –±—É–¥—É—Ç —Å–∏–ª—å–Ω–æ —à—É–º–µ—Ç—å –∏ —É—Ö—É–¥—à–∞—Ç—å MAE.\n",
    "- –î–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∏–Ω–≥—Ä–∏–¥–∏–µ–Ω—Ç–æ–≤ –≤ –±–ª—é–¥–µ, –∞ —Ç–∞–∫–∂–µ –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∏–Ω–≥—Ä–∏–¥–∏–µ–Ω—Ç—É –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ü–æ—ç—Ç–æ–º—É —Ç–æ–ª–∫—É –æ—Ç –Ω–∏—Ö –Ω–µ–º–Ω–æ–≥–æ...\n",
    "- –ú–∞—Å—Å—É –±–ª—é–¥ —Ç–æ–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–µ–º –∫–∞–∫ —Ç–µ–Ω–∑–æ—Ä, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫.\n",
    "- –°—Ç–∞–≤–∫—É –±–æ–ª—å—à–µ –¥–µ–ª–∞–µ–º –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å–ª–∞–±–∞—è, –ø–æ—Å–∫–æ–ª—å–∫—É —Ü–≤–µ—Ç–∞ –≤–∞–∂–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±–ª—é–¥, –ø—Ä–æ–¥—É–∫—Ç–æ–≤.\n",
    "- –í —Ö–æ–¥–µ –æ–±—É—á–µ–Ω–∏—è –≤—ã–≤–æ–¥–∏—Ç—å—Å—è –±—É–¥—É—Ç –º–µ—Ç—Ä–∏–∫–∏ MAE –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–µ–π –æ—Ç–¥–µ–ª—å–Ω–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ef6e5",
   "metadata": {},
   "source": [
    "# –≠—Ç–∞–ø 2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17326b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Projects/MultimodalModel/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# src/config.py\n",
    "class Config:\n",
    "    # –ú–æ–¥–µ–ª–∏\n",
    "    TEXT_MODEL_NAME = \"bert-base-uncased\"\n",
    "    IMAGE_MODEL_NAME = \"tf_efficientnet_b4\"\n",
    "\n",
    "    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 30\n",
    "    HIDDEN_DIM = 512\n",
    "    DROPOUT = 0.2\n",
    "    TEXT_LR = 1e-5  # LR –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
    "    IMAGE_LR = 1e-4  # LR –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    HEAD_LR = 1e-3  # LR –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
    "    SEED = 42\n",
    "\n",
    "    # –ü—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "    SAVE_PATH = \"best_model.pth\"\n",
    "\n",
    "# src/dataset.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import timm\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, dishes_df, ingredients_df, config, ds_type=\"train\", target_mean=None, target_std=None, mass_mean=None, mass_std=None, use_log_target=False):\n",
    "        self.dishes = dishes_df.reset_index(drop=True)\n",
    "        self.ingredients = ingredients_df\n",
    "        self.config = config\n",
    "        self.transforms = get_transforms(config, ds_type)\n",
    "        self._fix_names()\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∞—Ä–≥–µ—Ç–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–≤–æ–∑–º–æ–∂–Ω–æ –≤ –ª–æ–≥-—à–∫–∞–ª–µ)\n",
    "        self.mean = target_mean\n",
    "        self.std = target_std\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Å—ã\n",
    "        self.mass_mean = mass_mean\n",
    "        self.mass_std = mass_std\n",
    "        self.use_log_target = use_log_target\n",
    "\n",
    "    def _fix_names(self):\n",
    "        id_to_name = dict(zip(self.ingredients[\"id\"], self.ingredients[\"ingr\"]))\n",
    "        def map_ingrs(s):\n",
    "            if pd.isna(s):\n",
    "                return []\n",
    "            ids = [int(i.replace(\"ingr_\", \"\")) for i in s.split(\";\")]\n",
    "            return [id_to_name.get(i, i) for i in ids]\n",
    "        self.dishes[\"ingredients\"] = self.dishes[\"ingredients\"].apply(map_ingrs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dishes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dishes.iloc[idx]\n",
    "        ingredients = \", \".join([i for i in row['ingredients']])\n",
    "        text = f\"List of ingredients: {ingredients}\\nTotal mass: {row['total_mass']} grams.\"\n",
    "        image_path = f\"data/images/{row['dish_id']}/rgb.png\"\n",
    "        img = Image.open(image_path)\n",
    "        img_np = np.array(img)\n",
    "        aug = self.transforms(image=img_np)\n",
    "        img_tensor = aug[\"image\"]\n",
    "        raw_label = row[\"total_calories\"]\n",
    "        if self.use_log_target:\n",
    "            lab = np.log1p(max(0.0, raw_label))\n",
    "        else:\n",
    "            lab = float(raw_label)\n",
    "        label_norm = (lab - self.mean) / (self.std + 1e-9)\n",
    "        mass = float(row[\"total_mass\"])\n",
    "        mass_norm = (mass - (self.mass_mean if self.mass_mean is not None else 0.0)) / ((self.mass_std if self.mass_std is not None else 1.0) + 1e-9)\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"image\": img_tensor,\n",
    "            \"label\": torch.tensor(label_norm, dtype=torch.float32),\n",
    "            \"mass\": torch.tensor(mass_norm, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def get_transforms(config, ds_type=\"train\"):\n",
    "    cfg = timm.get_pretrained_cfg(config.IMAGE_MODEL_NAME)\n",
    "    max_side = max(cfg.input_size[1], cfg.input_size[2])\n",
    "    if ds_type == \"train\":\n",
    "        return A.Compose([\n",
    "            A.SmallestMaxSize(max_size=max_side, p=1.0),\n",
    "            A.CenterCrop(height=cfg.input_size[1], width=cfg.input_size[2]),\n",
    "            A.HorizontalFlip(p=0.1),\n",
    "            A.Affine(translate_percent=0.05, scale=(0.9, 1.1), rotate=(-15, 15), p=0.6),\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.6),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.HueSaturationValue(p=0.1),\n",
    "            A.Normalize(mean=cfg.mean, std=cfg.std),\n",
    "            A.pytorch.transforms.ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.SmallestMaxSize(max_size=max_side, p=1.0),\n",
    "            A.CenterCrop(height=cfg.input_size[1], width=cfg.input_size[2]),\n",
    "            A.Normalize(mean=cfg.mean, std=cfg.std),\n",
    "            A.pytorch.transforms.ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_length=128):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "    labels = torch.stack([item[\"label\"] for item in batch])\n",
    "    masses = torch.stack([item[\"mass\"] for item in batch])\n",
    "    tok = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return {\n",
    "        \"image\": images,\n",
    "        \"label\": labels,\n",
    "        \"mass\": masses,\n",
    "        \"input_ids\": tok[\"input_ids\"],\n",
    "        \"attention_mask\": tok[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# src/model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import timm\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * mask, dim=1) / torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "\n",
    "def freeze_all(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_last_n_layers(model, n):\n",
    "    layers = None\n",
    "    if hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
    "        layers = model.encoder.layer\n",
    "    else:\n",
    "        base = getattr(model, \"base_model\", None)\n",
    "        if base is not None and hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "            layers = base.encoder.layer\n",
    "\n",
    "    if layers is None:\n",
    "        return\n",
    "\n",
    "    total = len(layers)\n",
    "    for layer in layers[max(0, total - n):]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(config.TEXT_MODEL_NAME)\n",
    "        self.image_model = timm.create_model(config.IMAGE_MODEL_NAME, pretrained=True, num_classes=0)\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(self.text_model.config.hidden_size, config.HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(config.HIDDEN_DIM),\n",
    "            nn.Dropout(config.DROPOUT)\n",
    "        )\n",
    "        self.image_proj = nn.Sequential(\n",
    "            nn.Linear(self.image_model.num_features, config.HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(config.HIDDEN_DIM),\n",
    "            nn.Dropout(config.DROPOUT)\n",
    "        )\n",
    "        fused_dim = config.HIDDEN_DIM * 3 + 1\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(fused_dim, config.HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(config.HIDDEN_DIM),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.HIDDEN_DIM, config.HIDDEN_DIM // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.HIDDEN_DIM // 2, 1)\n",
    "        )\n",
    "\n",
    "        # === PARTIAL FREEZE TEXT =============================\n",
    "        freeze_all(self.text_model)\n",
    "        unfreeze_last_n_layers(self.text_model, n=6)\n",
    "\n",
    "        # === PARTIAL FREEZE IMAGE ===========================\n",
    "        freeze_all(self.image_model)\n",
    "        if hasattr(self.image_model, \"blocks\"):\n",
    "            for block in getattr(self.image_model, \"blocks\")[-5:]:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.image_model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image, mass=None,\n",
    "                mask_text=False, mask_image=False):\n",
    "        # TEXT\n",
    "        if not mask_text:\n",
    "            t = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            t_emb = mean_pooling(t, attention_mask)\n",
    "            t_emb = self.text_proj(t_emb)\n",
    "        else:\n",
    "            t_emb = torch.zeros((image.size(0), self.text_proj[0].out_features), device=image.device)\n",
    "\n",
    "        # IMAGE\n",
    "        if not mask_image:\n",
    "            img_feat = self.image_model(image)\n",
    "            i_emb = self.image_proj(img_feat)\n",
    "        else:\n",
    "            i_emb = torch.zeros((image.size(0), self.image_proj[0].out_features), device=image.device)\n",
    "\n",
    "        interaction = t_emb * i_emb\n",
    "        if mass is None:\n",
    "            mass = torch.zeros((image.size(0), 1), device=image.device)\n",
    "        else:\n",
    "            mass = mass.view(-1, 1)\n",
    "\n",
    "        fused = torch.cat([t_emb, i_emb, interaction, mass], dim=1)\n",
    "        return self.regressor(fused).squeeze(1)\n",
    "\n",
    "# src/utils.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from src.model import MultimodalModel\n",
    "from src.dataset import collate_fn\n",
    "from transformers import AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "\n",
    "def validate(model, loader, device, mask_text=False, mask_image=False, use_log_target=False):\n",
    "    model.eval()\n",
    "    mae_sum = 0.0\n",
    "    count = 0\n",
    "    mean = loader.dataset.mean\n",
    "    std = loader.dataset.std\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            preds = model(\n",
    "                input_ids=batch[\"input_ids\"].to(device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                image=batch[\"image\"].to(device),\n",
    "                mass=batch[\"mass\"].to(device),\n",
    "                mask_text=mask_text,\n",
    "                mask_image=mask_image\n",
    "            )\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            # –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ —Ç–æ–π –∂–µ —à–∫–∞–ª–µ, –≤ –∫–æ—Ç–æ—Ä–æ–π –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∏\n",
    "            preds = preds * std + mean\n",
    "            labels = labels * std + mean\n",
    "\n",
    "            if use_log_target:\n",
    "                # —Å–µ–π—á–∞—Å preds –∏ labels –≤ log1p-—à–∫–∞–ª–µ, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –¥–ª—è MAE\n",
    "                preds_orig = torch.expm1(preds).clamp(min=0.0)\n",
    "                labels_orig = torch.expm1(labels).clamp(min=0.0)\n",
    "                mae_sum += torch.sum(torch.abs(preds_orig - labels_orig)).item()\n",
    "                count += labels_orig.size(0)\n",
    "            else:\n",
    "                mae_sum += torch.sum(torch.abs(preds - labels)).item()\n",
    "                count += labels.size(0)\n",
    "    return mae_sum / count\n",
    "\n",
    "def train(config, train_ds, val_ds, use_log_target=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.TEXT_MODEL_NAME)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer),\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer),\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    model = MultimodalModel(config).to(device)\n",
    "\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ requires_grad\n",
    "    # --- —Å–æ–±—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ –≥—Ä—É–ø–ø–∞–º ---\n",
    "    text_params = []\n",
    "    image_params = []\n",
    "    head_params = []\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        # –ø—Ä–∏–º–µ—Ä–Ω–æ –ø–æ –∏–º–µ–Ω–∏ —Ä–∞–∑–¥–µ–ª—è–µ–º\n",
    "        if name.startswith(\"text_model\"):\n",
    "            text_params.append(p)\n",
    "        elif name.startswith(\"image_model\"):\n",
    "            image_params.append(p)\n",
    "        else:\n",
    "            head_params.append(p)\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    if len(text_params) > 0:\n",
    "        optimizer_grouped_parameters.append({\"params\": text_params, \"lr\": config.TEXT_LR})\n",
    "    if len(image_params) > 0:\n",
    "        optimizer_grouped_parameters.append({\"params\": image_params, \"lr\": config.IMAGE_LR})\n",
    "    if len(head_params) > 0:\n",
    "        optimizer_grouped_parameters.append({\"params\": head_params, \"lr\": config.HEAD_LR})\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, weight_decay=1e-3, eps=1e-8)\n",
    "    total_steps = len(train_loader) * config.EPOCHS\n",
    "    num_warmup_steps = int(0.2 * total_steps)  # 10% warmup\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=len(train_loader) * config.EPOCHS\n",
    "    )\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    scaler = amp.GradScaler()  # mixed precision\n",
    "\n",
    "    best_mae = 1e9\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"image\": batch[\"image\"].to(device),\n",
    "                \"mass\": batch[\"mass\"].to(device)\n",
    "            }\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            with amp.autocast(enabled=(device==\"cuda\")):\n",
    "                preds = model(**inputs)\n",
    "                loss = criterion(preds, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        val_mae = validate(model, val_loader, device, mask_text=False, mask_image=False, use_log_target=use_log_target)\n",
    "        mae_no_text = validate(model, val_loader, device, mask_text=True, mask_image=False, use_log_target=use_log_target)\n",
    "        mae_no_image = validate(model, val_loader, device, mask_text=False, mask_image=True, use_log_target=use_log_target)\n",
    "        print(f\"–≠–ø–æ—Ö–∞ {epoch+1}/{config.EPOCHS}. Train Loss: {total_loss/len(train_loader):.4f}. –û–±—â–µ–µ MAE: {val_mae:.2f}. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {mae_no_text:.2f}. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {mae_no_image:.2f}\")\n",
    "        if val_mae < best_mae:\n",
    "            best_mae = val_mae\n",
    "            torch.save(model.state_dict(), config.SAVE_PATH)\n",
    "            print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: {val_mae:.2f}\")\n",
    "    print(f\"–ë—ã–ª–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –ª—É—á—à–µ–µ MAE: {best_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0148d2",
   "metadata": {},
   "source": [
    "# –≠—Ç–∞–ø 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572cfe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Projects/MultimodalModel/src/utils.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()  # mixed precision\n",
      "/home/user/Projects/MultimodalModel/src/utils.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=(device==\"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–≠–ø–æ—Ö–∞ 1/30. Train Loss: 0.4627. –û–±—â–µ–µ MAE: 161.61. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 160.24. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 157.87\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 161.61\n",
      "–≠–ø–æ—Ö–∞ 2/30. Train Loss: 0.4293. –û–±—â–µ–µ MAE: 144.76. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 154.26. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 137.88\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 144.76\n",
      "–≠–ø–æ—Ö–∞ 3/30. Train Loss: 0.3635. –û–±—â–µ–µ MAE: 124.29. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 147.92. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 121.72\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 124.29\n",
      "–≠–ø–æ—Ö–∞ 4/30. Train Loss: 0.3004. –û–±—â–µ–µ MAE: 108.22. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 141.21. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 120.08\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 108.22\n",
      "–≠–ø–æ—Ö–∞ 5/30. Train Loss: 0.2539. –û–±—â–µ–µ MAE: 97.67. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 134.64. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 128.61\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 97.67\n",
      "–≠–ø–æ—Ö–∞ 6/30. Train Loss: 0.2138. –û–±—â–µ–µ MAE: 91.07. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 128.91. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 118.70\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 91.07\n",
      "–≠–ø–æ—Ö–∞ 7/30. Train Loss: 0.1855. –û–±—â–µ–µ MAE: 87.38. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 127.82. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 115.41\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 87.38\n",
      "–≠–ø–æ—Ö–∞ 8/30. Train Loss: 0.1652. –û–±—â–µ–µ MAE: 82.83. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 125.57. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 117.91\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 82.83\n",
      "–≠–ø–æ—Ö–∞ 9/30. Train Loss: 0.1456. –û–±—â–µ–µ MAE: 79.02. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 121.40. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 126.21\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 79.02\n",
      "–≠–ø–æ—Ö–∞ 10/30. Train Loss: 0.1351. –û–±—â–µ–µ MAE: 76.81. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 120.66. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 134.92\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 76.81\n",
      "–≠–ø–æ—Ö–∞ 11/30. Train Loss: 0.1184. –û–±—â–µ–µ MAE: 77.35. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 116.62. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 128.71\n",
      "–≠–ø–æ—Ö–∞ 12/30. Train Loss: 0.1172. –û–±—â–µ–µ MAE: 69.42. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 120.68. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 113.80\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 69.42\n",
      "–≠–ø–æ—Ö–∞ 13/30. Train Loss: 0.1107. –û–±—â–µ–µ MAE: 69.19. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 116.10. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 148.97\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 69.19\n",
      "–≠–ø–æ—Ö–∞ 14/30. Train Loss: 0.1006. –û–±—â–µ–µ MAE: 68.79. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 111.05. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 129.43\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 68.79\n",
      "–≠–ø–æ—Ö–∞ 15/30. Train Loss: 0.0963. –û–±—â–µ–µ MAE: 67.16. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 111.37. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 128.17\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 67.16\n",
      "–≠–ø–æ—Ö–∞ 16/30. Train Loss: 0.0933. –û–±—â–µ–µ MAE: 62.38. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 104.69. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 113.43\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 62.38\n",
      "–≠–ø–æ—Ö–∞ 17/30. Train Loss: 0.0874. –û–±—â–µ–µ MAE: 61.91. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 106.46. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 110.58\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 61.91\n",
      "–≠–ø–æ—Ö–∞ 18/30. Train Loss: 0.0863. –û–±—â–µ–µ MAE: 56.14. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 107.80. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 86.62\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 56.14\n",
      "–≠–ø–æ—Ö–∞ 19/30. Train Loss: 0.0827. –û–±—â–µ–µ MAE: 57.07. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 105.25. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 81.76\n",
      "–≠–ø–æ—Ö–∞ 20/30. Train Loss: 0.0753. –û–±—â–µ–µ MAE: 57.57. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 98.61. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 76.42\n",
      "–≠–ø–æ—Ö–∞ 21/30. Train Loss: 0.0687. –û–±—â–µ–µ MAE: 54.16. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 95.62. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 78.12\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 54.16\n",
      "–≠–ø–æ—Ö–∞ 22/30. Train Loss: 0.0730. –û–±—â–µ–µ MAE: 55.39. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 100.96. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 72.21\n",
      "–≠–ø–æ—Ö–∞ 23/30. Train Loss: 0.0705. –û–±—â–µ–µ MAE: 52.53. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 97.59. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 71.42\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 52.53\n",
      "–≠–ø–æ—Ö–∞ 24/30. Train Loss: 0.0663. –û–±—â–µ–µ MAE: 51.58. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 92.65. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 70.13\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 51.58\n",
      "–≠–ø–æ—Ö–∞ 25/30. Train Loss: 0.0625. –û–±—â–µ–µ MAE: 54.23. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 89.84. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 69.75\n",
      "–≠–ø–æ—Ö–∞ 26/30. Train Loss: 0.0610. –û–±—â–µ–µ MAE: 59.88. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 84.87. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 92.51\n",
      "–≠–ø–æ—Ö–∞ 27/30. Train Loss: 0.0613. –û–±—â–µ–µ MAE: 50.50. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 87.32. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 67.38\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 50.50\n",
      "–≠–ø–æ—Ö–∞ 28/30. Train Loss: 0.0533. –û–±—â–µ–µ MAE: 48.95. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 85.24. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 65.95\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 48.95\n",
      "–≠–ø–æ—Ö–∞ 29/30. Train Loss: 0.0516. –û–±—â–µ–µ MAE: 49.56. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 79.05. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 71.48\n",
      "–≠–ø–æ—Ö–∞ 30/30. Train Loss: 0.0517. –û–±—â–µ–µ MAE: 46.56. MAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 84.15. MAE –¥–ª—è —Ç–µ–∫—Å—Ç–∞: 65.65\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å MAE: 46.56\n",
      "–ë—ã–ª–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –ª—É—á—à–µ–µ MAE: 46.56\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.dataset import MultimodalDataset\n",
    "from src.utils import train\n",
    "from src.config import Config\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "\n",
    "    dishes = pd.read_csv(\"data/dish.csv\")\n",
    "    ingredients = pd.read_csv(\"data/ingredients.csv\")\n",
    "\n",
    "    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –æ—Ç—Å–µ—á—å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã\n",
    "    dishes = dishes[dishes[\"total_calories\"] < 750].reset_index(drop=True)\n",
    "    dishes = dishes[dishes[\"total_mass\"] < 900]\n",
    "\n",
    "    train_df = dishes[dishes[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "    # test_df = dishes[dishes[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º log1p —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–∞—Ä–≥–µ—Ç–∞ ‚Äî —ç—Ç–æ —á–∞—Å—Ç–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–π —Å–∫–æ—à–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "    use_log = True\n",
    "\n",
    "    if use_log:\n",
    "        train_targets = np.log1p(train_df[\"total_calories\"].clip(lower=0).values)\n",
    "    else:\n",
    "        train_targets = train_df[\"total_calories\"].values\n",
    "\n",
    "    train_mean = train_targets.mean()\n",
    "    train_std = train_targets.std()\n",
    "\n",
    "    # –ú–∞—Å—Å–∞ –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —á–∏—Å–ª–æ–≤–∞—è —Ñ–∏—á–∞\n",
    "    mass_vals = train_df[\"total_mass\"].values\n",
    "    mass_mean = mass_vals.mean()\n",
    "    mass_std = mass_vals.std()\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        train_df, \n",
    "        test_size=0.2,  \n",
    "        random_state=42,  \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    train_ds = MultimodalDataset(train_df, ingredients, cfg, ds_type=\"train\",\n",
    "                                 target_mean=train_mean, target_std=train_std,\n",
    "                                 mass_mean=mass_mean, mass_std=mass_std,\n",
    "                                 use_log_target=use_log)\n",
    "    \n",
    "    val_ds = MultimodalDataset(val_df, ingredients, cfg, ds_type=\"test\",\n",
    "                               target_mean=train_mean, target_std=train_std,\n",
    "                               mass_mean=mass_mean, mass_std=mass_std,\n",
    "                               use_log_target=use_log)\n",
    "\n",
    "    train(cfg, train_ds, val_ds, use_log_target=use_log)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2770142",
   "metadata": {},
   "source": [
    "# –≠—Ç–∞–ø 4. –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a1a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –±–ª—é–¥ –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞:\n",
      "============================================================\n",
      "–¢–µ–∫—Å—Ç: List of ingredients: white rice\n",
      "Total mass: 43.0 grams.\n",
      "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: data/images/dish_1558635523/rgb.png\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 78.48\n",
      "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: 55.90\n",
      "–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 22.58 –∫–∞–ª–æ—Ä–∏–π\n",
      "============================================================\n",
      "–¢–µ–∫—Å—Ç: List of ingredients: eggplant, chicken thighs, mushroom, vinegar, cherry tomatoes, lime, mixed greens, spinach (cooked), garlic, shallots, lemon juice, onions, olive oil, pepper, quinoa, wine, carrot, cheese pizza, salt, lettuce, cucumbers, squash\n",
      "Total mass: 188.0 grams.\n",
      "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: data/images/dish_1562099134/rgb.png\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 276.96\n",
      "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: 271.64\n",
      "–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 5.32 –∫–∞–ª–æ—Ä–∏–π\n",
      "============================================================\n",
      "–¢–µ–∫—Å—Ç: List of ingredients: apple\n",
      "Total mass: 116.0 grams.\n",
      "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: data/images/dish_1558639440/rgb.png\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 69.36\n",
      "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: 60.32\n",
      "–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 9.04 –∫–∞–ª–æ—Ä–∏–π\n",
      "============================================================\n",
      "–¢–µ–∫—Å—Ç: List of ingredients: olive oil, blueberries, vinaigrette, walnuts, salt, cucumbers, onions, olives, feta cheese, arugula, vinegar, chard, pepper, cherry tomatoes, bell peppers, lettuce, mustard\n",
      "Total mass: 89.0 grams.\n",
      "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: data/images/dish_1561574815/rgb.png\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 125.70\n",
      "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: 74.85\n",
      "–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 50.85 –∫–∞–ª–æ—Ä–∏–π\n",
      "============================================================\n",
      "–¢–µ–∫—Å—Ç: List of ingredients: honeydew melons\n",
      "Total mass: 107.0 grams.\n",
      "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: data/images/dish_1558030289/rgb.png\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 54.14\n",
      "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: 38.52\n",
      "–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 15.62 –∫–∞–ª–æ—Ä–∏–π\n",
      "============================================================\n",
      "\n",
      "–°—Ä–µ–¥–Ω–∏–π MAE –ø–æ 5 –ø—Ä–∏–º–µ—Ä–∞–º: 20.68 –∫–∞–ª–æ—Ä–∏–π\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.dataset import MultimodalDataset\n",
    "from src.model import MultimodalModel\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T    \n",
    "from src.config import Config\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "def test_single_prediction(config, image_path, text, model_path, target_std, target_mean):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    model = MultimodalModel(config).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.TEXT_MODEL_NAME)\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "    text_encoded = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "    with torch.no_grad():\n",
    "        pred = model(\n",
    "            input_ids=text_encoded['input_ids'].to(device),\n",
    "            attention_mask=text_encoded['attention_mask'].to(device),\n",
    "            image=image_tensor\n",
    "        )\n",
    "    pred = pred * target_std + target_mean\n",
    "    pred = torch.expm1(pred).clamp(min=0.0)\n",
    "\n",
    "    print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "    print(f\"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}\")\n",
    "    print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {pred.item():.2f}\")\n",
    "    return pred.item()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config()\n",
    "\n",
    "    dishes = pd.read_csv(\"data/dish.csv\")\n",
    "    ingredients = pd.read_csv(\"data/ingredients.csv\")\n",
    "\n",
    "    test_df = dishes[dishes[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "    test_ds = MultimodalDataset(test_df, ingredients, cfg, ds_type=\"test\")\n",
    "    train_targets = np.log1p(test_df[\"total_calories\"].clip(lower=0).values)\n",
    "    test_mean = train_targets.mean()\n",
    "    test_std = train_targets.std()\n",
    "\n",
    "    # –í—ã–±–∏—Ä–∞–µ–º 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    random_indices = random.sample(range(len(test_ds)), min(5, len(test_ds)))\n",
    "\n",
    "    print(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –±–ª—é–¥ –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    mae_errors = []\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        item = test_ds[idx]\n",
    "\n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –∏–∑ DataFrame –¥–ª—è –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "        row = test_df.iloc[idx]\n",
    "\n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é\n",
    "        image_path = f\"data/images/{row['dish_id']}/rgb.png\"\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "        prediction = test_single_prediction(\n",
    "            config=cfg,\n",
    "            image_path=image_path,\n",
    "            text=item[\"text\"],\n",
    "            model_path=\"best_model.pth\", \n",
    "            target_std=test_std,\n",
    "            target_mean=test_mean,\n",
    "        )\n",
    "\n",
    "        # –í—ã–≤–æ–¥–∏–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏—Å—Ç–∏–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
    "        true_calories = row['total_calories']\n",
    "        error = abs(prediction - true_calories)\n",
    "        mae_errors.append(error)\n",
    "        print(f\"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {true_calories:.2f}\")\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {error:.2f} –∫–∞–ª–æ—Ä–∏–π\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    final_mae = sum(mae_errors) / len(mae_errors)\n",
    "    print(f\"\\n–°—Ä–µ–¥–Ω–∏–π MAE –ø–æ {len(mae_errors)} –ø—Ä–∏–º–µ—Ä–∞–º: {final_mae:.2f} –∫–∞–ª–æ—Ä–∏–π\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
